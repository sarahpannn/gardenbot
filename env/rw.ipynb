{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment can provide a variety of rewards to guide the bot's decision-making process and encourage desired behaviors. For instance, positive rewards can be given when the bot successfully identifies and removes weeds, waters the plants optimally, or performs efficient soil nutrient management. Rewards can also be tied to achieving specific gardening goals, such as promoting healthy plant growth, maximizing crop yield, or maintaining an aesthetically pleasing garden layout. On the other hand, negative rewards or penalties can be assigned when the bot damages plants, fails to address plant diseases promptly, or exhibits inefficient resource utilization. By designing a reward system that aligns with the goals of gardening, the autonomous bot can learn to navigate the environment, adapt its actions, and develop effective gardening strategies that lead to a flourishing garden ecosystem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR\n",
    "\n",
    "* The game: We want our robot to learn to garden (with a number of additional constraints)\n",
    "* The problem: We can't just have our robot flail around and try everything until it somehow stumbles upon the optimal solution\n",
    "* The solution: We create a method with which to evaluate the current environment\n",
    "* The analogy: In chess, there are many ways to evaluate the board long before either opponent wins. This reward function does that but for gardening.\n",
    "* A potential confusion: This is different from the value produced by the critic model. The reward here is produced by the environment and is non-changing, objective, whatever makes the most sense. The value produced by the critic model is gradient based and is analogous to the chess player's brain. It gets better by practice and evaluates its current chess playing method to eventually maximize the reward provided by the environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A potential recipe\n",
    "\n",
    "note: this would be relevant for the simulation; we'd develop a new one for the actual thing\n",
    "\n",
    "    1. Legality of state:\n",
    "        - x for x overlapping area\n",
    "    \n",
    "    2. Conservation of water:\n",
    "        -1 for each water used\n",
    "        \n",
    "    3. Diversity of garden:\n",
    "        + x for x > 2 plant species\n",
    "        0 for 2 species\n",
    "        - x for x < 2 plant species\n",
    "    \n",
    "    4. Density of garden\n",
    "        +% for %density > 75\n",
    "        -% for %density < 75\n",
    "\n",
    "    5. Health\n",
    "        penalty for dying plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util_bs.quadtree import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class reward():\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment # this will be in the form of a state dict\n",
    "        \n",
    "        \n",
    "    def _overlap_penalty(self):\n",
    "        plant_list = self.environment[\"plants\"]\n",
    "        qt = QuadTree(0, 0, 183, 91)\n",
    "        \n",
    "        for plant in plant_list:\n",
    "            qt.insert(QuadTreeNode(plant.x_coord - plant.radius, \n",
    "                                         plant.y_coord - plant.radius, \n",
    "                                         plant.x_coord + plant.radius, \n",
    "                                         plant.y_coord + plant.radius))\n",
    "        overlap_area = 0\n",
    "        for i in range(len(plant_list)):\n",
    "            for j in range(i+1, len(plant_list)):\n",
    "                overlap = qt.calculate_overlap(plant_list[i], plant_list[j])\n",
    "                if overlap > 0: overlap_area += overlap\n",
    "        return overlap_area / (183 * 91) - 1\n",
    "\n",
    "    def _water_penalty(self, dampening_factor=0.01):\n",
    "        return dampening_factor * self.environment[\"water\"]\n",
    "    \n",
    "    def _diversity_score(self):\n",
    "        plant_list = self.environment[\"plants\"]\n",
    "        species_list = []\n",
    "        for plant in plant_list:\n",
    "            if plant.species not in species_list:\n",
    "                species_list.append(plant.species)\n",
    "        if len(plant_list) == 0:\n",
    "            return 0\n",
    "        if len(species_list) / len(plant_list) < 0.5:\n",
    "            return  -len(species_list) / len(plant_list)\n",
    "        return len(species_list) / len(plant_list)\n",
    "    \n",
    "    def _death_penalty(self):\n",
    "        penalty = 0\n",
    "        plant_list = self.environment[\"plants\"]\n",
    "        for plant in plant_list:\n",
    "            if not plant.alive:\n",
    "                penalty += 1\n",
    "        return - penalty / len(plant_list)\n",
    "    \n",
    "    def get_composite_score(self):\n",
    "        unnormalized_score = self._overlap_penalty() + self._water_penalty() + self._diversity_score() - self._death_penalty()\n",
    "        return unnormalized_score / 4\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
